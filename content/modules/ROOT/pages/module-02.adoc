= Module 2: Deploy MCP Server & Configure Lightspeed

== Overview

This module is **REQUIRED** before the End-to-End Self-Healing demo. You'll deploy the MCP Server and configure OpenShift Lightspeed to enable AI-powered cluster management.

**What you'll accomplish:**

* Deploy the Cluster Health MCP Server
* Configure LLM provider (OpenAI, existing Lightspeed, or vLLM)
* Create OLSConfig to connect Lightspeed to MCP Server
* Verify the integration is working
* Learn the 7 MCP tools available for cluster management

[IMPORTANT]
====
This module must be completed before Module 3 (End-to-End Self-Healing), as the self-healing demo relies on Lightspeed to interact with the platform.
====

== Prerequisites

Before proceeding, ensure:

* ✅ Completed Module 0 and Module 1
* ✅ Platform is deployed (via AgnosticD workload)
* ✅ You have an OpenAI API key OR existing Lightspeed cluster OR Hugging Face account
* ✅ Admin access to the cluster

Verify platform is running:
[source,bash,role=execute,subs="attributes+"]
----
oc get pods -n {namespace}
oc get inferenceservices -n {namespace}
----

== Part 1: Verify the MCP Server

The MCP (Model Context Protocol) Server enables OpenShift Lightspeed to interact with your self-healing platform.

=== Step 1.1: Verify MCP Server Deployment

The MCP Server should already be deployed by the AgnosticD workload:

[source,bash,role=execute,subs="attributes+"]
----
oc get deployment mcp-server -n {namespace}
oc get service mcp-server -n {namespace}
----

**Expected output:**
[source]
----
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
mcp-server   1/1     1            1           2d

NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
mcp-server   ClusterIP   172.30.118.176   <none>        8080/TCP   2d
----

=== Step 1.2: Check MCP Server Health

[source,bash,role=execute,subs="attributes+"]
----
oc run test-curl --image=registry.access.redhat.com/ubi9/ubi-minimal:latest \
  --rm -i --restart=Never -n {namespace} -- \
  curl -s http://mcp-server:8080/health
----

**Expected output:**
[source]
----
OK
----

=== Step 1.3: View MCP Server Logs

Check the MCP server logs to see registered tools and resources:

[source,bash,role=execute,subs="attributes+"]
----
oc logs deployment/mcp-server -n {namespace} --tail=30
----

**Expected log output:**
[source]
----
2026/02/19 14:34:01 Total tools registered: 12
2026/02/19 14:34:01 Total resources registered: 4
2026/02/19 14:34:01 Total prompts registered: 6
2026/02/19 14:34:01 MCP Server initialized: openshift-cluster-health v0.1.0
2026/02/19 14:34:01 MCP Server listening on 0.0.0.0:8080
----

[NOTE]
====
The MCP Server provides **12 tools**, **4 resources**, and **6 prompts** that Lightspeed can use for cluster management. You'll learn about these in Part 6.
====

== Part 2: Configure LLM Provider

OpenShift Lightspeed requires an LLM provider. Choose ONE option based on your environment:

[cols="1,2,2,1"]
|===
|Option |Description |Best For |Setup Time

|**A: OpenAI**
|Use OpenAI's cloud API (manual setup)
|Quick start, reliable responses
|~5 min

|**B: Existing Lightspeed**
|Cluster already has Lightspeed configured
|Pre-configured environments (Azure, watsonx)
|~10 min

|**C: Deploy vLLM**
|Self-host Llama 3.2 1B in your cluster
|No external dependencies, learning experience
|~20 min
|===

=== Option A: OpenAI (Recommended for Quick Start)

**Step A.1: Get OpenAI API Key**

1. Visit https://platform.openai.com/api-keys
2. Create new API key
3. Copy the key (starts with `sk-proj-`)

**Step A.2: Create Secret**

[source,bash,role=execute,subs="attributes+"]
----
oc create secret generic openai-api-key \
  -n openshift-lightspeed \
  --from-literal=apitoken='sk-proj-YOUR_KEY_HERE'
----

[NOTE]
====
Continue to Part 3, Option A for OLSConfig creation.
====

=== Option B: Existing Lightspeed Cluster

**For clusters with pre-configured Lightspeed** (Azure OpenAI, IBM watsonx, etc.)

**Step B.1: Verify Existing OLSConfig**

[source,bash,role=execute,subs="attributes+"]
----
# Check if OLSConfig exists
oc get olsconfig cluster -n openshift-lightspeed

# View current configuration
oc get olsconfig cluster -o yaml | less
----

Look for existing `spec.llm.providers` section showing your LLM provider.

**Step B.2: Identify Provider Type**

Common configurations:

**Azure OpenAI:**
[source,yaml]
----
spec:
  llm:
    providers:
      - name: Azure
        type: azure_openai
        url: 'https://your-instance.openai.azure.com/'
        credentialsSecretRef:
          name: azure-api-keys
----

**IBM watsonx:**
[source,yaml]
----
spec:
  llm:
    providers:
      - name: watsonx
        type: watsonx
        url: 'https://us-south.ml.cloud.ibm.com'
        credentialsSecretRef:
          name: watsonx-api-keys
----

[NOTE]
====
Your existing LLM configuration will be preserved. We'll only add MCP Server integration in Part 3.
Continue to Part 3, Option B.
====

=== Option C: Deploy vLLM (Self-Hosted LLM)

**Deploy vLLM inference server in your cluster** - no external API keys required!

[IMPORTANT]
====
**Requirements:**
- 8Gi+ available memory
- Persistent storage (10Gi for model caching)
- Network access to Hugging Face Hub
- ~10-15 minutes for initial model download
====

**Step C.1: Accept Llama 3.2 License**

1. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct
2. Sign in to Hugging Face (create account if needed)
3. Click "Agree and access repository"
4. Create access token: https://huggingface.co/settings/tokens
   - Token type: Read
   - Copy the token (starts with `hf_`)

**Step C.2: Create Hugging Face Token Secret**

[source,bash,role=execute,subs="attributes+"]
----
# Replace with your actual token
oc create secret generic huggingface-token \
  -n {namespace} \
  --from-literal=token='hf_YOUR_TOKEN_HERE'
----

**Step C.3: Deploy vLLM Server**

[source,bash,role=execute,subs="attributes+"]
----
cat <<EOF | oc apply -f -
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vllm-model-cache
  namespace: {namespace}
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-server
  namespace: {namespace}
  labels:
    app: vllm-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-server
  template:
    metadata:
      labels:
        app: vllm-server
    spec:
      containers:
      - name: vllm
        image: quay.io/vllm/vllm-openai:latest
        command:
          - python3
          - -m
          - vllm.entrypoints.openai.api_server
          - --model
          - meta-llama/Llama-3.2-1B-Instruct
          - --port
          - "8000"
          - --host
          - "0.0.0.0"
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: huggingface-token
              key: token
        - name: HF_HOME
          value: "/models"
        ports:
        - containerPort: 8000
          protocol: TCP
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"
        volumeMounts:
        - name: model-cache
          mountPath: /models
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 300
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 320
          periodSeconds: 30
      volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: vllm-model-cache
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-server
  namespace: {namespace}
spec:
  selector:
    app: vllm-server
  ports:
  - port: 8000
    targetPort: 8000
    protocol: TCP
    name: http
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: vllm-server
  namespace: {namespace}
spec:
  to:
    kind: Service
    name: vllm-server
  port:
    targetPort: http
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
EOF
----

**Step C.4: Monitor vLLM Deployment**

[source,bash,role=execute,subs="attributes+"]
----
# Watch pod startup (model download takes 5-10 minutes)
oc get pods -n {namespace} -l app=vllm-server -w
----

Open another terminal and watch logs:
[source,bash,role=execute,subs="attributes+"]
----
# Check logs for model download progress
oc logs -n {namespace} deployment/vllm-server -f
----

**Expected log output:**
[source]
----
INFO: Downloading meta-llama/Llama-3.2-1B-Instruct...
INFO: Downloaded 483 MB / 1.2 GB...
INFO: Model loaded successfully
INFO: Started server process
----

**Step C.5: Test vLLM Server**

[source,bash,role=execute,subs="attributes+"]
----
# Get the route URL
VLLM_URL=$(oc get route vllm-server -n {namespace} -o jsonpath='{.spec.host}')
echo "vLLM URL: https://$VLLM_URL"

# Test with a simple completion
curl -k https://$VLLM_URL/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.2-1B-Instruct",
    "prompt": "OpenShift is",
    "max_tokens": 20
  }'
----

**Expected response:**
[source,json]
----
{
  "id": "cmpl-xxx",
  "object": "text_completion",
  "created": 1234567890,
  "model": "meta-llama/Llama-3.2-1B-Instruct",
  "choices": [{
    "text": " a container orchestration platform...",
    "index": 0,
    "finish_reason": "length"
  }]
}
----

✅ If you see a valid JSON response with generated text, vLLM is working!

[NOTE]
====
Continue to Part 3, Option C for OLSConfig creation.
====

[IMPORTANT]
====
**Secret key name:** The secret key MUST be named `apitoken` for OpenAI (Option A). For vLLM (Option C), the Hugging Face token uses key name `token`.
====

== Part 3: Create OLSConfig

The OLSConfig custom resource connects OpenShift Lightspeed to your LLM provider and MCP Server.

[tabs]
====
Option A: OpenAI::
+
--
**For Option A (Manual OpenAI Setup)**

**Step A.3: Create OLSConfig**

[source,bash,role=execute,subs="attributes+"]
----
cat <<EOF | oc apply -f -
apiVersion: ols.openshift.io/v1alpha1
kind: OLSConfig
metadata:
  name: cluster
spec:
  featureGates:
    - MCPServer

  llm:
    providers:
      - name: openai
        type: openai
        url: "https://api.openai.com/v1"
        credentialsSecretRef:
          name: openai-api-key
        models:
          - name: gpt-5-mini
          - name: gpt-4o

  mcpServers:
    - name: cluster-health
      streamableHTTP:
        url: http://mcp-server.{namespace}.svc:8080/mcp
        timeout: 30
        enableSSE: true

  ols:
    defaultModel: gpt-5-mini
    defaultProvider: openai
    deployment:
      replicas: 1
    conversationCache:
      type: postgres

  console:
    enabled: true
EOF
----

Continue to verification steps below.
--

Option B: Existing Lightspeed::
+
--
**For Option B (Existing Lightspeed Cluster)**

Instead of creating a new OLSConfig, we'll **patch** the existing one to add MCP Server integration.

**Step B.3: Backup Current OLSConfig**

[source,bash,role=execute,subs="attributes+"]
----
oc get olsconfig cluster -o yaml > /tmp/olsconfig-backup.yaml
----

**Step B.4: Add MCP Server Configuration**

[source,bash,role=execute,subs="attributes+"]
----
# Add MCPServer feature gate and mcpServers section
oc patch olsconfig cluster --type=merge -p '
spec:
  featureGates:
    - MCPServer
  mcpServers:
    - name: cluster-health
      streamableHTTP:
        url: http://mcp-server.{namespace}.svc:8080/mcp
        timeout: 30
        enableSSE: true
'
----

[IMPORTANT]
====
This patch **preserves** your existing LLM provider configuration while adding MCP Server support.
====

**Step B.5: Verify Merged Configuration**

[source,bash,role=execute,subs="attributes+"]
----
# View the updated OLSConfig
oc get olsconfig cluster -o yaml | less

# Verify both sections exist:
# 1. Your existing llm.providers (should be unchanged)
# 2. New mcpServers section (should show cluster-health)
# 3. featureGates should include MCPServer
----

Continue to verification steps below.
--

Option C: vLLM::
+
--
**For Option C (Deploy vLLM)**

**Step C.6: Create Dummy API Secret**

vLLM doesn't require authentication, but OLSConfig requires a credentialsSecretRef:

[source,bash,role=execute,subs="attributes+"]
----
oc create secret generic vllm-api-key \
  -n openshift-lightspeed \
  --from-literal=apitoken='none'
----

**Step C.7: Create OLSConfig for vLLM**

[source,bash,role=execute,subs="attributes+"]
----
# Get vLLM route URL
VLLM_URL=$(oc get route vllm-server -n {namespace} -o jsonpath='{.spec.host}')
echo "Using vLLM URL: https://$VLLM_URL"

cat <<EOF | oc apply -f -
apiVersion: ols.openshift.io/v1alpha1
kind: OLSConfig
metadata:
  name: cluster
spec:
  featureGates:
    - MCPServer

  llm:
    providers:
      - name: vllm-local
        type: rhoai_vllm
        url: "https://$VLLM_URL/v1"
        credentialsSecretRef:
          name: vllm-api-key
        models:
          - name: meta-llama/Llama-3.2-1B-Instruct

  mcpServers:
    - name: cluster-health
      streamableHTTP:
        url: http://mcp-server.{namespace}.svc:8080/mcp
        timeout: 30
        enableSSE: true

  ols:
    defaultModel: meta-llama/Llama-3.2-1B-Instruct
    defaultProvider: vllm-local
    deployment:
      replicas: 1
    conversationCache:
      type: postgres

  console:
    enabled: true
EOF
----

[NOTE]
====
**Provider Type:** We use `rhoai_vllm` for OpenShift AI vLLM integration. This tells Lightspeed to use the vLLM OpenAI-compatible API format.
====

Continue to verification steps below.
--
====

=== Verify OLSConfig (All Options)

[source,bash,role=execute,subs="attributes+"]
----
# Check OLSConfig exists
oc get olsconfig cluster

# Watch for all conditions to become Ready
oc get olsconfig cluster -o jsonpath='{.status.conditions[*].type}' && echo
oc get olsconfig cluster -o jsonpath='{.status.conditions[*].status}' && echo
----

**Expected conditions:** `ConsolePluginReady`, `CacheReady`, `ApiReady = True`

=== Verify Lightspeed Pods

[source,bash,role=execute,subs="attributes+"]
----
oc get pods -n openshift-lightspeed
----

Wait for `lightspeed-app-server` to be Running (~2-3 minutes).

== Part 4: Test the Integration

[NOTE]
====
**For vLLM Users (Option C):** Llama 3.2 1B is a smaller model than GPT-4o. Responses may be:

- More concise
- Occasionally less accurate for complex queries
- Faster (lower latency)

This is expected behavior - the model is optimized for efficiency over maximum accuracy.
====

=== Step 4.1: Access Lightspeed via Console

. Open OpenShift Console: {openshift_console_url}
. Look for the **Lightspeed chatbot icon** in the top right
. Click to open the chat interface

=== Step 4.2: Test MCP Tool Queries

Try these queries in Lightspeed:

[cols="1,2"]
|===
|Query |Expected Response

|"What is the cluster health?"
|Uses `get-cluster-health` tool, shows pod/node status

|"List pods in {namespace}"
|Uses `list-pods` tool, shows running pods

|"What ML models are available?"
|Uses `list-models` tool, shows InferenceServices

|"Are there any anomalies?"
|Uses `analyze-anomalies` tool, runs ML detection
|===

=== Step 4.3: Verify MCP Server Tool Calls

Watch MCP Server logs while asking questions:

[source,bash,role=execute,subs="attributes+"]
----
oc logs -f deployment/mcp-server -n {namespace}
----

You should see `Tool invoked:` messages when Lightspeed uses MCP tools.

== Part 5: Understanding OLSConfig

Now that Lightspeed is working, let's understand the OLSConfig in detail.

=== Complete OLSConfig Reference

The OLSConfig is a **cluster-scoped singleton** that configures OpenShift Lightspeed. Here's a comprehensive example showing all supported provider types:

[source,yaml]
----
apiVersion: ols.openshift.io/v1alpha1
kind: OLSConfig
metadata:
  name: cluster  # MUST be 'cluster' (cluster-scoped singleton)
spec:
  # Enable MCP Server integration
  featureGates:
    - MCPServer

  # LLM Provider configuration (configure one or multiple)
  llm:
    providers:
      # OpenAI (GPT-5, GPT-4o)
      - name: openai
        type: openai
        url: "https://api.openai.com/v1"
        credentialsSecretRef:
          name: openai-api-key
        models:
          - name: gpt-5-mini       # Latest efficient model (default)
          - name: gpt-4o           # Previous generation multimodal
          - name: gpt-4            # Stable baseline

      # Google Gemini (OpenAI-compatible endpoint)
      - name: google
        type: openai
        url: "https://generativelanguage.googleapis.com/v1beta/openai"
        credentialsSecretRef:
          name: google-api-key
        models:
          - name: gemini-2.0-flash-exp  # Fast & efficient
          - name: gemini-1.5-pro        # Advanced reasoning

      # Azure OpenAI
      - name: azure
        type: azure_openai
        url: "https://YOUR_RESOURCE.openai.azure.com"
        credentialsSecretRef:
          name: azure-openai-key
        models:
          - name: gpt-4o           # Requires matching deployment name
          - name: gpt-4o-mini      # High efficiency

      # IBM watsonx (BAM)
      - name: watsonx
        type: watsonx
        url: "https://us-south.ml.cloud.ibm.com"
        credentialsSecretRef:
          name: watsonx-api-key
        models:
          - name: ibm/granite-13b-chat-v2  # Enterprise model

      # vLLM (Self-hosted - RHOAI integration)
      - name: vllm-local
        type: rhoai_vllm
        url: "https://vllm-server-{namespace}.apps.cluster.example.com/v1"
        credentialsSecretRef:
          name: vllm-api-key
        models:
          - name: meta-llama/Llama-3.2-1B-Instruct  # Self-hosted

  # MCP Server configuration (integrates with self-healing platform)
  mcpServers:
    - name: cluster-health
      streamableHTTP:
        url: http://mcp-server.{namespace}.svc:8080/mcp
        timeout: 30
        enableSSE: true

  # OLS deployment configuration
  ols:
    defaultModel: gpt-5-mini        # Choose your default model
    defaultProvider: openai         # Choose your default provider
    deployment:
      replicas: 1
    conversationCache:
      type: postgres                # REQUIRED: Only 'postgres' is supported

  # Console plugin configuration
  console:
    enabled: true
----

=== View Current Configuration

[source,bash,role=execute,subs="attributes+"]
----
# View the complete OLSConfig
oc get olsconfig cluster -o yaml

# View just the LLM providers
oc get olsconfig cluster -o jsonpath='{.spec.llm.providers[*].name}' && echo

# View MCP servers
oc get olsconfig cluster -o jsonpath='{.spec.mcpServers[*].name}' && echo
----

=== Supported LLM Provider Types

[cols="1,1,2,2"]
|===
|Provider |Type Value |Authentication |Notes

|**OpenAI**
|`openai`
|API key in secret
|GPT-5-mini, GPT-4o recommended

|**Google Gemini**
|`openai`
|API key in secret
|Uses OpenAI-compatible endpoint

|**Azure OpenAI**
|`azure_openai`
|API key + endpoint
|Requires Azure deployment name

|**IBM watsonx**
|`watsonx`
|API key
|Granite models for enterprise

|**vLLM (RHOAI)**
|`rhoai_vllm`
|Optional (dummy secret)
|Self-hosted, no external API

|**IBM BAM**
|`bam`
|API key
|IBM Research platform
|===

=== Secret Format Requirements

All LLM provider secrets must use the key name **`apitoken`**:

[source,bash,role=execute,subs="attributes+"]
----
# Correct format
oc create secret generic openai-api-key \
  -n openshift-lightspeed \
  --from-literal=apitoken='sk-proj-...'

# WRONG - will fail validation
oc create secret generic openai-api-key \
  -n openshift-lightspeed \
  --from-literal=api_key='sk-proj-...'  # ❌ Wrong key name
----

=== Key Configuration Points

**Cluster-scoped singleton:**
- OLSConfig name MUST be `cluster`
- Only one OLSConfig can exist per cluster
- No namespace field in metadata

**MCP Server integration:**
- Requires `MCPServer` feature gate
- Uses internal service URL for cluster-health MCP server
- Timeout of 30 seconds recommended for ML model calls

**Conversation cache:**
- Only `postgres` type is supported
- Automatically provisioned by Lightspeed operator
- Stores conversation history for context

**Multiple providers:**
- Configure multiple providers for redundancy
- Set `defaultProvider` and `defaultModel` for primary use
- Users can switch providers via console UI

== Part 6: The 7 MCP Tools

The MCP Server exposes these tools that Lightspeed can call:

=== Tool Reference

[cols="1,2,2"]
|===
|Tool |Description |Returns

|`get-cluster-health`
|Check namespace status, pods, ML models
|Health summary with metrics

|`list-pods`
|Query pods with filtering
|Pod list with details

|`analyze-anomalies`
|Call ML models for detection
|Anomaly detection results + recommendations

|`trigger-remediation`
|Apply fixes via Coordination Engine
|Remediation status and ID

|`list-incidents`
|Query historical incidents
|Incident data with resolution info

|`get-model-status`
|Check KServe InferenceService health
|Model status and endpoints

|`list-models`
|List available ML model catalog
|Model names and capabilities
|===

=== How Tools Are Called

When you ask Lightspeed a question, it:

. Parses your natural language intent
. Selects the appropriate MCP tool(s)
. Calls the tool via MCP protocol
. Receives structured JSON response
. Formats a human-readable reply

**Example Flow:**
[source]
----
You: "What's the cluster health?"
     │
     ▼
Lightspeed: Intent = "get health status"
            Tool = "get-cluster-health"
     │
     ▼
MCP Server: GET /tools/get-cluster-health
            Params: {namespace: "self-healing-platform"}
     │
     ▼
Coordination Engine: Query Kubernetes API
                     Check InferenceServices
     │
     ▼
Response: {
  "status": "healthy",
  "pods": [...],
  "models": [...]
}
     │
     ▼
Lightspeed: "✅ Cluster Health Summary for self-healing-platform:
            Healthy Components (4): ..."
----

== Part 7: LightspeedClient Python API (Optional)

For programmatic access, you can use the LightspeedClient.

=== Run Python Examples in a Container

All Python examples are available in a pre-built container image with scripts and dependencies included.

[NOTE]
====
The container image was automatically built during platform deployment and is ready to use.
====

[NOTE]
====
If you get an error like "pod already exists", delete the existing pod first:
[source,bash,role=execute,subs="attributes+"]
----
oc delete pod lightspeed-examples -n {namespace}
----
====

[source,bash,role=execute,subs="attributes+"]
----
# Run the container interactively
oc run lightspeed-examples \
  --image=image-registry.openshift-image-registry.svc:5000/{namespace}/lightspeed-python-examples:latest \
  -n {namespace} \
  --rm -it -- bash

# Inside the container, scripts are ready to use
ls -l *.py
python lightspeed_client.py --help
----

Browse source code: https://github.com/KubeHeal/self-healing-workshop/tree/main/examples/python[GitHub Examples Directory^]

=== Basic Usage

[source,python]
----
import requests
from typing import Dict, Any

class LightspeedClient:
    """Client for OpenShift Lightspeed communication."""
    
    def __init__(self, server_url: str, timeout: int = 30):
        self.server_url = server_url
        self.timeout = timeout
        self.session = requests.Session()
    
    def query(self, question: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Send query to Lightspeed."""
        try:
            payload = {
                'question': question,
                'context': context or {}
            }
            
            response = self.session.post(
                f"{self.server_url}/query",
                json=payload,
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                return {'error': response.text}
        except Exception as e:
            return {'error': str(e)}
    
    def get_recommendations(self, issue_type: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Get AI recommendations for issue."""
        try:
            payload = {
                'issue_type': issue_type,
                'context': context
            }
            
            response = self.session.post(
                f"{self.server_url}/recommendations",
                json=payload,
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                return {'error': response.text}
        except Exception as e:
            return {'error': str(e)}
----

=== Sending Queries

[source,python]
----
# Initialize client
OLS_SERVER_URL = 'http://ols-server:8000'
client = LightspeedClient(OLS_SERVER_URL)

# Send a query
response = client.query(
    "How do I troubleshoot high CPU usage in OpenShift pods?",
    context={'namespace': 'self-healing-platform'}
)

print(response)
----

=== Getting Recommendations

[source,python]
----
# Get recommendations for a specific issue
issue_context = {
    'pod_name': 'coordination-engine-0',
    'namespace': 'self-healing-platform',
    'cpu_usage': 85,
    'memory_usage': 72,
    'restart_count': 2
}

recommendations = client.get_recommendations(
    'high_resource_usage',
    issue_context
)

print(recommendations)
----

== Part 8: Extracting Actionable Insights (Optional)

Process Lightspeed responses to extract actionable information:

[source,python]
----
from datetime import datetime
from typing import Dict, Any

def extract_insights(response: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extract actionable insights from Lightspeed response.
    
    Args:
        response: Lightspeed response
    
    Returns:
        Extracted insights with confidence scores
    """
    if 'error' in response:
        return {'status': 'error', 'message': response['error']}
    
    # Extract key information
    insights = {
        'status': 'success',
        'timestamp': datetime.now().isoformat(),
        'answer': response.get('answer', ''),
        'confidence': response.get('confidence', 0.0),
        'sources': response.get('sources', []),
        'recommendations': response.get('recommendations', [])
    }
    
    return insights

# Usage
query_response = client.query("Why is my pod failing?")
insights = extract_insights(query_response)

print(f"Confidence: {insights['confidence']:.2%}")
print(f"Recommendations: {insights['recommendations']}")
----

== Part 9: Integration Patterns (Optional)

=== Run Integration Pattern Examples

Use the container image to run integration patterns:

[source,bash,role=execute,subs="attributes+"]
----
# Run the container
oc run lightspeed-examples \
  --image=image-registry.openshift-image-registry.svc:5000/{namespace}/lightspeed-python-examples:latest \
  -n {namespace} \
  --rm -it -- bash

# Inside the container, run patterns
python pattern_alert_response.py --help
python pattern_batch_analysis.py --namespace {namespace}
python pattern_capacity_planning.py --output /tmp/report.json && cat /tmp/report.json
----

View all scripts: https://github.com/KubeHeal/self-healing-workshop/tree/main/examples/python[GitHub Examples Directory^]

=== Pattern 1: Automated Alert Response

[source,python]
----
def handle_prometheus_alert(alert: dict):
    """Respond to Prometheus alert with Lightspeed."""
    
    # Build context from alert
    context = {
        'alert_name': alert['labels']['alertname'],
        'namespace': alert['labels'].get('namespace', 'default'),
        'severity': alert['labels'].get('severity', 'warning'),
        'description': alert['annotations'].get('description', ''),
    }
    
    # Query Lightspeed for analysis
    client = LightspeedClient('http://ols-server:8000')
    
    analysis = client.query(
        f"Analyze this alert and suggest remediation: {context['description']}",
        context=context
    )
    
    # Extract recommendations
    insights = extract_insights(analysis)
    
    if insights['confidence'] > 0.8:
        # High confidence - auto-remediate
        return {
            'action': 'auto_remediate',
            'recommendations': insights['recommendations']
        }
    else:
        # Low confidence - escalate to human
        return {
            'action': 'escalate',
            'analysis': insights
        }
----

=== Pattern 2: Batch Analysis

[source,python]
----
import pandas as pd

def analyze_pod_fleet(namespace: str) -> pd.DataFrame:
    """Analyze all pods in namespace with Lightspeed."""
    
    client = LightspeedClient('http://ols-server:8000')
    
    # Get pod list
    pods_response = client.query(
        f"List all pods in {namespace} with their status",
        context={'namespace': namespace}
    )
    
    # Analyze each problematic pod
    results = []
    for pod in pods_response.get('pods', []):
        if pod['status'] != 'Running':
            analysis = client.query(
                f"Why is pod {pod['name']} not running?",
                context={'namespace': namespace, 'pod': pod['name']}
            )
            
            results.append({
                'pod': pod['name'],
                'status': pod['status'],
                'analysis': analysis.get('answer', ''),
                'confidence': analysis.get('confidence', 0)
            })
    
    return pd.DataFrame(results)
----

=== Pattern 3: Capacity Planning Report

[source,python]
----
def generate_capacity_report(namespace: str) -> dict:
    """Generate capacity planning report with predictions."""
    
    client = LightspeedClient('http://ols-server:8000')
    
    # Get current usage
    current = client.query(
        f"What's the current resource usage in {namespace}?",
        context={'namespace': namespace}
    )
    
    # Get predictions for key times
    predictions = {}
    for time in ['9 AM', '12 PM', '3 PM', '6 PM']:
        pred = client.query(
            f"What will resource usage be at {time}?",
            context={'namespace': namespace}
        )
        predictions[time] = pred
    
    # Get capacity recommendations
    recommendations = client.get_recommendations(
        'capacity_planning',
        {'namespace': namespace, 'timeframe': '7 days'}
    )
    
    return {
        'current_usage': current,
        'predictions': predictions,
        'recommendations': recommendations,
        'generated_at': datetime.now().isoformat()
    }
----

== Part 10: Hands-On Exercise (Optional)

Let's create a simple monitoring script that continuously checks cluster health.

=== Quick Start: Run the Monitor

Run the cluster monitoring script using the container image:

[source,bash,role=execute,subs="attributes+"]
----
# Run the monitoring script (press Ctrl+C to stop)
oc run lightspeed-monitor \
  --image=image-registry.openshift-image-registry.svc:5000/{namespace}/lightspeed-python-examples:latest \
  -n {namespace} \
  --rm -it -- python monitor_cluster.py
----

**Expected output:**
[source]
----
======================================================================
OpenShift Lightspeed Cluster Health Monitor
======================================================================
Server URL: http://lightspeed-app-server.openshift-lightspeed.svc:8080
Namespace: self-healing-platform
Check interval: 60 seconds
======================================================================

[2026-02-19 14:30:00] Checking cluster health...
  ✅ All systems healthy
     The self-healing-platform namespace is running smoothly...
----

Press `Ctrl+C` to stop monitoring.

[TIP]
====
You can also run the container interactively to explore all scripts:

[source,bash,role=execute,subs="attributes+"]
----
oc run lightspeed-examples \
  --image=image-registry.openshift-image-registry.svc:5000/{namespace}/lightspeed-python-examples:latest \
  -n {namespace} \
  --rm -it -- bash
----
====

=== Alternative: Create the Script Manually

If you prefer to create the script yourself, here's the complete code:

=== Create the Script

[source,bash,role=execute,subs="attributes+"]
----
cat > /tmp/monitor_with_lightspeed.py << 'EOF'
#!/usr/bin/env python3
"""
Simple monitoring script using Lightspeed.
"""

import requests
import time
from datetime import datetime

OLS_SERVER_URL = 'http://ols-server:8000'
NAMESPACE = 'self-healing-platform'
CHECK_INTERVAL = 60  # seconds

def query_lightspeed(question, context=None):
    """Send query to Lightspeed."""
    try:
        response = requests.post(
            f"{OLS_SERVER_URL}/query",
            json={'question': question, 'context': context or {}},
            timeout=30
        )
        return response.json() if response.ok else {'error': response.text}
    except Exception as e:
        return {'error': str(e)}

def check_cluster_health():
    """Check cluster health via Lightspeed."""
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] Checking cluster health...")
    
    response = query_lightspeed(
        f"Are there any issues in the {NAMESPACE} namespace?",
        {'namespace': NAMESPACE}
    )
    
    if 'error' in response:
        print(f"  ⚠️  Error: {response['error']}")
        return False
    
    # Check for issues in response
    answer = response.get('answer', '').lower()
    if any(word in answer for word in ['healthy', 'no issues', 'running']):
        print(f"  ✅ All systems healthy")
        return True
    else:
        print(f"  ⚠️  Potential issues detected:")
        print(f"     {response.get('answer', 'Unknown')[:200]}")
        return False

def main():
    """Main monitoring loop."""
    print("=" * 60)
    print("Lightspeed Cluster Monitor")
    print(f"Namespace: {NAMESPACE}")
    print(f"Check interval: {CHECK_INTERVAL}s")
    print("=" * 60)
    
    while True:
        try:
            check_cluster_health()
            time.sleep(CHECK_INTERVAL)
        except KeyboardInterrupt:
            print("\n\nMonitoring stopped.")
            break

if __name__ == '__main__':
    main()
EOF
----

=== Run the Monitor (Optional)

[source,bash]
----
# This would run in the Jupyter workbench or a pod with Python
python /tmp/monitor_with_lightspeed.py
----

== Part 11: Understanding the Architecture

=== Complete Data Flow

[source]
----
┌─────────────────────────────────────────────────────────────────┐
│                     User Interface Layer                         │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────────┐  │
│  │ OCP Console │  │ Python API  │  │ Custom Integrations     │  │
│  └──────┬──────┘  └──────┬──────┘  └───────────┬─────────────┘  │
│         │                │                     │                 │
│         └────────────────┴─────────────────────┘                 │
│                          │                                       │
│                          ▼                                       │
├─────────────────────────────────────────────────────────────────┤
│                     AI/LLM Layer                                │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │           OpenShift Lightspeed (OLS Server)             │   │
│  │  • Natural language understanding                       │   │
│  │  • MCP tool selection                                   │   │
│  │  • Response formatting                                  │   │
│  └────────────────────────┬────────────────────────────────┘   │
│                           │ MCP Protocol                        │
│                           ▼                                     │
├─────────────────────────────────────────────────────────────────┤
│                     Tool Layer                                  │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              MCP Server (Go)                            │   │
│  │  • 7 tools exposed via MCP protocol                     │   │
│  │  • Routes requests to Coordination Engine               │   │
│  │  • Returns structured JSON responses                    │   │
│  └────────────────────────┬────────────────────────────────┘   │
│                           │ REST API                            │
│                           ▼                                     │
├─────────────────────────────────────────────────────────────────┤
│                     Orchestration Layer                         │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │           Coordination Engine (Go)                       │   │
│  │  • Queries Prometheus for metrics                       │   │
│  │  • Calls KServe ML models                               │   │
│  │  • Applies remediation to cluster                       │   │
│  │  • Tracks incidents and history                         │   │
│  └─────────┬─────────────────┬─────────────────┬───────────┘   │
│            │                 │                 │                 │
│            ▼                 ▼                 ▼                 │
├─────────────────────────────────────────────────────────────────┤
│                     Data/ML Layer                               │
│  ┌───────────────┐ ┌───────────────┐ ┌───────────────────────┐ │
│  │   Prometheus  │ │ KServe Models │ │   Kubernetes API      │ │
│  │   (metrics)   │ │ (inference)   │ │   (cluster ops)       │ │
│  └───────────────┘ └───────────────┘ └───────────────────────┘ │
└─────────────────────────────────────────────────────────────────┘
----

== Summary

In this module, you accomplished:

* ✅ **Deployed MCP Server** - Verified the cluster-health MCP server is running
* ✅ **Configured LLM Provider** - Set up OpenAI, Google Gemini, or Anthropic
* ✅ **Created OLSConfig** - Connected Lightspeed to MCP Server
* ✅ **Tested Integration** - Verified Lightspeed can use MCP tools
* ✅ **Learned the 7 MCP Tools** - Understand what tools are available
* ✅ **Understood Architecture** - Complete data flow from query to response

[NOTE]
====
**You're now ready for Module 3: End-to-End Self-Healing!**

With Lightspeed configured, you can now interact with the self-healing platform using natural language.
====

== Resources

=== Documentation

* https://docs.openshift.com/container-platform/latest/lightspeed/[OpenShift Lightspeed Documentation]
* https://modelcontextprotocol.io/[Model Context Protocol Specification]
* {platform_repo}[Platform GitHub Repository]

=== Architecture Decision Records

* https://github.com/KubeHeal/openshift-aiops-platform/blob/main/docs/adrs/016-openshift-lightspeed-olsconfig-integration.md[ADR-016: OLSConfig Integration]
* https://github.com/KubeHeal/openshift-aiops-platform/blob/main/docs/adrs/036-go-based-standalone-mcp-server.md[ADR-036: Go-Based MCP Server]
* https://github.com/KubeHeal/openshift-aiops-platform/blob/main/docs/adrs/038-go-coordination-engine-migration.md[ADR-038: Go Coordination Engine]

=== Related Projects

* {mcp_server_repo}[MCP Server Repository]
* {coordination_engine_repo}[Coordination Engine Repository]

---

**Next: xref:module-03.adoc[Module 3: End-to-End Self-Healing with Lightspeed]**
