= Module 3: End-to-End Self-Healing with Lightspeed

== Overview

Imagine talking to your Kubernetes cluster like you would talk to a colleague:

* "Are there any failing pods?"
* "What will my memory usage be at 3 PM?"
* "Fix the broken pods automatically"

This is now possible with OpenShift Lightspeed connected to our self-healing platform. In this module, we'll explore how to interact with an already-deployed platform using just natural language.

[IMPORTANT]
====
**Prerequisite**: Complete xref:module-02.adoc[Module 2: Deploy MCP Server & Configure Lightspeed] before starting this module.
====

**What's already deployed:**

* ‚úÖ OpenShift Lightspeed (AI assistant) - _Configured in Module 2_
* ‚úÖ MCP Server (Go service - connects Lightspeed to cluster tools) - _Verified in Module 2_
* ‚úÖ Coordination Engine (Go service - orchestrates remediation)
* ‚úÖ KServe ML Models (anomaly detection + capacity forecasting)

[NOTE]
====
**Architecture Note**: The MCP Server and Coordination Engine are **Go services** for production performance. The notebooks are **Python** for ML/data science. You don't need to write Go code to use the platform!
====

== Part 1: Your First Conversation - Health Check

[NOTE]
====
**Building on Module 2:** In Module 2, you configured Lightspeed and tested basic queries using the 7 MCP tools:

* `get-cluster-health` - Check namespace pod/model health
* `list-pods` - Query pods with filtering
* `analyze-anomalies` - Run ML anomaly detection
* `trigger-remediation` - Apply automated fixes
* `list-incidents` - Query historical incidents
* `get-model-status` - Check KServe model health
* `list-models` - List ML model catalog

Now we'll use these tools for real troubleshooting workflows - all through natural language in the UI!
====

[IMPORTANT]
====
**About Lightspeed Responses in This Module:**

The example conversations below show **sample interactions** for demonstration purposes. Your actual Lightspeed responses **will differ** based on:

* **LLM Model:** GPT-4o, vLLM (Llama 3.2), Gemini, etc. generate different response styles
* **Your Cluster State:** Actual resource usage, pod counts, and issues you encounter
* **Deployed Apps:** What you deploy in Part 2 affects predictions and analysis results
* **Model Training Data:** ML models learn from YOUR cluster's Prometheus metrics

**The key is successful interaction**, not matching exact text. As long as Lightspeed understands your questions and provides relevant cluster information, you're successfully using the self-healing platform!
====

=== Open OpenShift Lightspeed

. Navigate to your OpenShift web console: {openshift_console_url}
. Click the **Lightspeed icon** (‚ú®) in the top-right corner
. The chat interface opens

=== Ask About Cluster Health

**You type:**
[source]
----
What's the health of the {namespace} namespace?
----

**Example response:**
[source]
----
‚úÖ Cluster Health Summary for {namespace}:

Healthy Components (4):
- coordination-engine: Running
- mcp-server: Running
- anomaly-detector-predictor: 2/2 Running
- predictive-analytics-predictor: 2/2 Running

ML Models Status:
- anomaly-detector: READY ‚úÖ
- predictive-analytics: READY ‚úÖ

Recent Activity:
- 0 incidents in the last hour
- All systems operational

Everything looks healthy!
----

üéØ **What just happened?** Lightspeed used the `get-cluster-health` MCP tool to query your cluster, check pod status, and verify ML models are ready.

== Part 2: Deploy a Sample Application

Let's deploy a simple Flask app that we'll use throughout the rest of this module.

[NOTE]
====
We use the Red Hat UBI Python image (`registry.access.redhat.com/ubi9/python-311`) because it's designed for OpenShift's security model, which runs containers as a random non-root UID.
====

[source,bash,role=execute,subs="attributes+"]
----
oc apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-flask-app
  namespace: {namespace}
spec:
  replicas: 2
  selector:
    matchLabels:
      app: sample-flask
  template:
    metadata:
      labels:
        app: sample-flask
    spec:
      containers:
      - name: web
        image: registry.access.redhat.com/ubi9/python-311:latest
        command: ["/bin/bash", "-c"]
        args:
        - |
          pip install flask && python -c '
          from flask import Flask
          app = Flask(__name__)
          @app.route("/")
          def hello():
              return "Hello from OpenShift!"
          app.run(host="0.0.0.0", port=8080)
          '
        ports:
        - containerPort: 8080
        resources:
          limits:
            memory: "256Mi"
            cpu: "200m"
          requests:
            memory: "128Mi"
            cpu: "100m"
EOF
----

Wait about 30 seconds for the pods to start, then use Lightspeed to verify:

**You type:**
[source]
----
Show me the sample-flask-app pods in the {namespace} namespace
----

**Example response:**
[source]
----
Found 2 pods for sample-flask-app:

1. sample-flask-app-xxx
   Status: Running ‚úÖ
   CPU: 45m / 200m (22%)
   Memory: 82Mi / 128Mi (64%)
   Restarts: 0

2. sample-flask-app-yyy
   Status: Running ‚úÖ
   CPU: 47m / 200m (23%)
   Memory: 79Mi / 128Mi (62%)
   Restarts: 0

Both pods are healthy!
----

== Part 3: Predicting Future Resource Usage

This is where the ML models shine! Let's ask about future resource usage.

=== Scenario 1: Predict Afternoon Usage

**You type:**
[source]
----
What will the CPU and memory usage be at 3 PM today?
----

**Example response:**
[source]
----
Confirmed prediction for 15:00 today (cluster-wide)

CPU: 58.0%
Memory: 58.6%
Model confidence: 85%

Current baseline (estimate):
- CPU: ~3.5%
- Memory: ~27.1%

Key notes:
- Big CPU jump predicted ‚Äî verify this isn't a metrics-collection gap
- Important metrics to check in Prometheus:
  - CPU: container_cpu_usage_seconds_total
  - Memory: container_memory_working_set_bytes

Quick commands:
  kubectl top nodes
  kubectl top pods -A

Next steps ‚Äî pick one:
1. I can run an anomaly scan for CPU/memory
2. I can calculate how many more pods you can safely schedule

Which would you like?
----

[NOTE]
====
üìä Your predictions will differ based on your cluster's learned patterns! The model trains on YOUR Prometheus metrics.
====

üéØ **What happened?** Lightspeed:

. Called the MCP `predict-resource-usage` tool
. The Coordination Engine queried Prometheus for current 24h rolling mean
. Extracted hour (15) and day of week from "3 PM today"
. Called the `predictive-analytics` KServe model
. Formatted a human-friendly response with recommendations

=== Scenario 2: Scaling Impact Prediction

**You type:**
[source]
----
If I scale sample-flask-app to 5 replicas, what will happen to memory usage?
----

**Example response:**
[source]
----
Scaling Impact Analysis:

Current State:
- Replicas: 2
- Memory per pod: ~80Mi
- Total memory: ~160Mi

Projected State (5 replicas):
- Replicas: 5 (+3)
- Memory per pod: ~82Mi
- Total memory: ~410Mi
- Namespace memory: 92.3% ‚ö†Ô∏è

‚ö†Ô∏è WARNING: Memory usage will approach critical threshold (95%)

üí° Recommendations:
1. Scale to 4 replicas instead (predicted: 86.7%)
2. Increase namespace memory limits by 20%
3. Monitor for 30 minutes after scaling

Would you like me to proceed with scaling?
----

=== Scenario 3: Capacity Planning

**You type:**
[source]
----
How many more pods can I run before hitting resource limits?
----

== Part 4: Breaking Things (On Purpose!)

Let's deploy a broken app to see self-healing in action.

=== Deploy a Memory-Starved Application

This app has a memory leak - each request allocates 10MB. With only 96Mi limit, it will OOMKill after a few requests.

[source,bash,role=execute,subs="attributes+"]
----
oc apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: broken-app
  namespace: {namespace}
spec:
  replicas: 2
  selector:
    matchLabels:
      app: broken
  template:
    metadata:
      labels:
        app: broken
    spec:
      containers:
      - name: web
        image: registry.access.redhat.com/ubi9/python-311:latest
        command: ["/bin/bash", "-c"]
        args:
        - |
          pip install flask && python -c '
          from flask import Flask
          app = Flask(__name__)
          data = []
          @app.route("/")
          def hello():
              data.append("x" * 1024 * 1024 * 10)  # Allocate 10MB per request
              return f"Allocated {len(data)} chunks ({len(data) * 10}MB total)"
          app.run(host="0.0.0.0", port=8080)
          '
        ports:
        - containerPort: 8080
        resources:
          limits:
            memory: "96Mi"    # Low - will OOMKill after ~3-4 requests
            cpu: "100m"
          requests:
            memory: "64Mi"
            cpu: "50m"
EOF
----

=== Trigger the Memory Leak

The app won't crash on its own - we need to trigger the memory leak by sending requests:

[source,bash,role=execute,subs="attributes+"]
----
# Wait for pods to start
sleep 30

# Hit the endpoint repeatedly to trigger memory leak
for i in {1..20}; do
  echo "Request $i:"
  oc exec -n {namespace} deployment/broken-app -- curl -s http://localhost:8080/ 2>/dev/null || echo "Pod crashed!"
  sleep 1
done
----

You should see output like:
[source]
----
Request 1: Allocated 1 chunks (10MB total)
Request 2: Allocated 2 chunks (20MB total)
...
Request 7: Allocated 7 chunks (70MB total)
Request 8: Pod crashed!
Request 9: Allocated 1 chunks (10MB total)  ‚Üê Pod restarted!
----

=== Watch It Fail with Lightspeed

**You type:**
[source]
----
Are there any failing pods in the {namespace} namespace?
----

**Example response:**
[source]
----
‚ö†Ô∏è Found 2 pods with issues:

1. broken-app-xxx
   Status: OOMKilled ‚ùå
   Restarts: 2
   Reason: Container exceeded memory limit (96Mi)
   Pattern: Crash loop - restarts when memory leak triggered

2. broken-app-yyy
   Status: OOMKilled ‚ùå
   Restarts: 1
   Reason: Container exceeded memory limit (96Mi)
   Pattern: Memory leak causes OOMKill under load

Both pods are experiencing Out Of Memory errors. The app has a
memory leak - each request allocates 10MB that isn't released.
The 96Mi limit is exceeded after ~7 requests.

Would you like me to analyze these for anomalies?
----

== Part 5: AI-Powered Anomaly Detection

**You type:**
[source]
----
Yes, analyze the broken-app pods for anomalies
----

**Example response:**
[source]
----
ML Analysis Complete (anomaly-detector model):

üî¥ ANOMALY DETECTED
Confidence: 95%

Root Cause:
- Type: Memory Leak
- Component: Application code
- Memory limit: 96Mi
- Pattern: Linear growth ~10MB/request until OOMKill
- Evidence: Memory never released between requests

Historical Context:
- 3 similar memory leak patterns in past 7 days
- Temporary fixes: increase limits 2-3x
- Permanent fixes: code review for unbounded arrays/caches

Impact:
- Service availability: 0% (both replicas failing)
- User impact: Complete outage
- Blast radius: Isolated to broken-app

üí° Recommended Fix:
IMMEDIATE (automated):
1. Increase memory limit to 256Mi (buys time)
2. Set memory request to 128Mi

PERMANENT (requires developer):
3. Review code: data array grows unbounded
4. Implement cleanup or bounded cache

Would you like me to apply the immediate fix?
----

üéØ **What happened?** The anomaly-detector ML model analyzed the pod metrics and crash patterns, identified this as a known resource exhaustion pattern.

== Part 6: Self-Healing in Action

**You type:**
[source]
----
Yes, fix it automatically
----

**Example response:**
[source]
----
Remediation Plan:

Target: deployment/broken-app
Action: Update Resource Limits

Changes:
  Memory: 96Mi  ‚Üí 256Mi (temporary fix)
  CPU:    100m  ‚Üí 200m

Note: This increases limits to prevent crashes, but the
root cause (memory leak in code) should still be fixed.

Applying via Coordination Engine...

‚úÖ Remediation Applied!
- ID: rem-20260112-001
- Status: SUCCESS
- Pods restarting with new limits...

Expected completion: ~45 seconds
----

=== Verify the Fix

After 45 seconds:

**You type:**
[source]
----
Are the broken-app pods healthy now?
----

**Example response:**
[source]
----
‚úÖ Remediation Successful!

1. broken-app-xxx
   Status: Running ‚úÖ
   CPU: 52m / 200m (26%)
   Memory: 87Mi / 256Mi (34%)
   Restarts: 0 (since fix)
   Uptime: 1m

2. broken-app-yyy
   Status: Running ‚úÖ
   CPU: 49m / 200m (24%)
   Memory: 84Mi / 256Mi (33%)
   Restarts: 0 (since fix)
   Uptime: 1m

Memory Analysis:
- Current: 85Mi average
- Peak: 95Mi (161Mi headroom with new limit)
- Stability: No restarts since remediation

‚ö†Ô∏è Note: The memory leak still exists - increased limits
buy time but code should be reviewed to fix the leak.
----

== Part 7: Understanding Historical Patterns

**You type:**
[source]
----
Show me recent incidents and how they were resolved
----

**Example response:**
[source]
----
Incident History (Last 24 Hours):

Total: 1 incident
Average resolution: 45 seconds

üìã Incident Details:

inc-20260112-001
- Time: 15:44:38 UTC
- Type: OOMKilled (Memory Leak)
- Pods: broken-app-xxx, broken-app-yyy
- Root cause: Application memory leak (~10MB/request)
- Trigger: Memory exceeded 96Mi limit after ~7 requests

Remediation:
- Action: Updated deployment resources (temporary fix)
- Applied: YES ‚úÖ
- Resolution time: 45s
- Changes: Memory 96Mi‚Üí256Mi, CPU 100m‚Üí200m
- Outcome: 100% success (pods stable)
- Follow-up: Code review recommended for memory leak

Pattern Recognition:
ML detected this as "memory leak" pattern, not just
"insufficient resources". Recommended both immediate
fix (increase limits) and permanent fix (code review).
----

== Part 8: More Conversations to Try

[NOTE]
====
These are **example conversation flows** showing possible interactions. Your actual conversations will vary based on your LLM model and cluster state. Use these as inspiration for questions to ask!
====

=== Troubleshooting

**Investigate slow performance:**
[source]
----
You: The sample-flask-app feels slow

Lightspeed:
Investigating...

Pod Analysis:
- sample-flask-app-xxx: CPU 185m/200m (92%) ‚ö†Ô∏è
- sample-flask-app-yyy: CPU 178m/200m (89%) ‚ö†Ô∏è

Root Cause: CPU saturation (throttling)
Recommendation: Scale to 3 replicas or increase CPU limits

Should I scale now?
----

**Proactive monitoring:**
[source]
----
You: Will any pods have issues in the next hour?

Lightspeed:
üîÆ Predictions for next 60 minutes:

Low Risk: 5 pods stable

‚ö†Ô∏è Medium Risk:
- sample-flask-app-xxx: CPU will reach 99% in ~45 min
- sample-flask-app-yyy: CPU will reach 97% in ~50 min

Recommendation: Scale to 3 replicas NOW to prevent issues

Should I apply preemptively?
----

=== Capacity Questions

[source]
----
You: What will memory usage be tomorrow at 9 AM?

Lightspeed:
Predicted memory at 09:00 tomorrow:
- 71.3% (current trend: -3.2% from today)
- Lower usage expected (morning hours)
- Confidence: 88%
----

[source]
----
You: Can I run 10 more medium-sized pods?

Lightspeed:
Capacity check for 10 medium pods (128Mi/200m):

Required resources:
- Memory: 1280Mi
- CPU: 2000m

Current availability:
- Memory: Only ~600Mi available
- CPU: ~800m available

‚ùå Cannot accommodate 10 pods
‚úÖ Can safely run 4 pods
‚ö†Ô∏è 5-6 pods possible but approaching limits
----

=== Model Health

[source]
----
You: Are the ML models working properly?

Lightspeed:
ML Model Status:

1. anomaly-detector
   Status: READY ‚úÖ
   Endpoint: http://anomaly-detector-predictor:8080
   Recent predictions: 47 (last hour)
   Average latency: 23ms
   Success rate: 100%

2. predictive-analytics
   Status: READY ‚úÖ
   Endpoint: http://predictive-analytics-predictor:8080
   Recent predictions: 32 (last hour)
   Average latency: 18ms
   Success rate: 100%

All models healthy and performing well!
----

== Part 9: Quick Reference

=== Health & Status
[source]
----
"What's the cluster health?"
"Are there any failing pods?"
"Show me pods in {namespace}"
"What's using the most memory?"
----

=== Predictions
[source]
----
"What will CPU be at 3 PM?"
"Predict memory usage tomorrow at 9 AM"
"What happens if I scale to 5 replicas?"
"How many more pods can I run?"
"Will I have capacity issues this weekend?"
----

=== Troubleshooting
[source]
----
"Why is pod X failing?"
"Analyze deployment Y for anomalies"
"What caused the OOMKilled errors?"
"Show me pods with high restarts"
"Will any pods fail in the next hour?"
----

=== Actions
[source]
----
"Fix the failing pods"
"Scale deployment X to 5 replicas"
"Increase memory for pod Y"
----

=== ML Models
[source]
----
"Are ML models healthy?"
"What models are available?"
"Check anomaly detector status"
----

=== History
[source]
----
"Show recent incidents"
"What's the average resolution time?"
"How many incidents happened today?"
----

== Part 10: Cleanup

Before moving to the next module, let's clean up the test deployments:

[source,bash,role=execute,subs="attributes+"]
----
oc delete deployment sample-flask-app broken-app -n {namespace}
----

== Summary

You've now explored:

* ‚úÖ Chatting with your cluster using natural language
* ‚úÖ Deploying sample workloads
* ‚úÖ Using ML models to predict resource usage
* ‚úÖ Detecting anomalies automatically
* ‚úÖ Triggering self-healing remediation
* ‚úÖ Understanding historical patterns

=== The Power of Natural Language Operations

**Traditional way:**
[source,bash]
----
kubectl get pods -n {namespace} | grep -v Running
kubectl describe pod broken-app-xxx
kubectl logs broken-app-xxx
kubectl edit deployment broken-app
# ... manually update limits ...
kubectl rollout status deployment/broken-app
----

**With Lightspeed:**
[source]
----
"Fix the broken pods"
----

=== Key Metrics

* Detection to fix: ~45 seconds
* Prediction accuracy: 92%+
* ML inference time: <100ms
* Zero manual intervention needed

== Next Steps

In **xref:module-03.adoc[Module 3: OpenShift Lightspeed Deep Dive]** (optional), you'll learn:

* OLSConfig resource configuration
* LightspeedClient Python API
* Processing AI responses programmatically
* Building custom MCP tools

---

**Happy chatting with your cluster! üöÄ**
