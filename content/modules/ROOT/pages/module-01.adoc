= Module 1: ML Model Training with Tekton

== Overview

This module demonstrates how to train and deploy machine learning models using Tekton pipelines. Automated model training ensures models stay current with cluster behavior, improving prediction accuracy and anomaly detection reliability.

**What you'll learn:**

* Train models manually with custom time windows
* Schedule automated weekly retraining
* Integrate real Prometheus metrics with synthetic data
* Validate model health before deployment
* Add your own custom models

== Quick Start: Train a Model

=== Manual Training with Default Settings (24h Data)

Train the anomaly detector with 24 hours of recent data:

[source,bash,role=execute]
----
oc create -f - <<EOF
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: train-anomaly-detector-
  namespace: {namespace}
spec:
  pipelineRef:
    name: model-training-pipeline
  params:
    - name: model-name
      value: "anomaly-detector"
    - name: notebook-path
      value: "notebooks/02-anomaly-detection/01-isolation-forest-implementation.ipynb"
    - name: data-source
      value: "prometheus"
    - name: training-hours
      value: "24"
    - name: inference-service-name
      value: "anomaly-detector"
    - name: git-url
      value: "{platform_repo}.git"
    - name: git-ref
      value: "main"
  timeout: 30m
EOF
----

Monitor the training progress:

[source,bash,role=execute]
----
# Watch pipeline execution
tkn pipelinerun logs -f -n {namespace}
----

[source,bash,role=execute]
----
# Check training job status
oc get notebookvalidationjobs -n {namespace}
----

[source,bash,role=execute]
----
# View model file
oc exec -n {namespace} deployment/model-troubleshooting-utilities -- \
  ls -lh /mnt/models/anomaly-detector/
----

=== Manual Training with Custom Time Window

Train the predictive analytics model with 30 days of data for capturing seasonal patterns:

[source,bash,role=execute]
----
oc create -f - <<EOF
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: train-predictive-analytics-
  namespace: {namespace}
spec:
  pipelineRef:
    name: model-training-pipeline
  params:
    - name: model-name
      value: "predictive-analytics"
    - name: notebook-path
      value: "notebooks/02-anomaly-detection/05-predictive-analytics-kserve.ipynb"
    - name: data-source
      value: "prometheus"
    - name: training-hours
      value: "720"
    - name: inference-service-name
      value: "predictive-analytics"
    - name: git-url
      value: "{platform_repo}.git"
    - name: git-ref
      value: "main"
  timeout: 45m
EOF
----

=== GPU-Accelerated Training with NotebookValidationJob

For faster training, use GPU resources:

[source,bash,role=execute]
----
oc create -f - <<'EOF'
apiVersion: mlops.mlops.dev/v1alpha1
kind: NotebookValidationJob
metadata:
  name: train-predictive-gpu
  namespace: {namespace}
  labels:
    model-name: predictive-analytics
spec:
  notebook:
    git:
      ref: main
      url: {platform_repo}.git
    path: notebooks/02-anomaly-detection/05-predictive-analytics-kserve.ipynb
  podConfig:
    containerImage: image-registry.openshift-image-registry.svc:5000/{namespace}/notebook-validator:latest
    env:
    - name: DATA_SOURCE
      value: prometheus
    - name: PROMETHEUS_URL
      value: https://prometheus-k8s.openshift-monitoring.svc:9091
    - name: TRAINING_HOURS
      value: "168"
    - name: MODEL_NAME
      value: predictive-analytics
    nodeSelector:
      nvidia.com/gpu.present: "true"
    resources:
      limits:
        cpu: "4"
        memory: 8Gi
        nvidia.com/gpu: "1"
      requests:
        cpu: "2"
        memory: 4Gi
        nvidia.com/gpu: "1"
    serviceAccountName: self-healing-workbench
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    volumeMounts:
    - mountPath: /mnt/models
      name: model-storage
    volumes:
    - name: model-storage
      persistentVolumeClaim:
        claimName: gpu-training-pvc
  timeout: 45m
EOF
----

[NOTE]
====
GPU nodes may not have CephFS drivers. Use `gpu-training-pvc` (GP3 storage class) instead of `model-storage-pvc` (CephFS) for GPU-accelerated training. Copy the model to CephFS after training completes.
====

== Training Time Windows

Choose the appropriate time window based on your use case:

[cols="1,1,2,2"]
|===
|Duration |Hours |Use Case |Example

|**1 day**
|24
|Quick iteration, development, testing
|Testing notebook changes

|**1 week**
|168
|Weekly retraining, production anomaly detection
|Anomaly detector scheduled training

|**30 days**
|720
|Initial training, seasonal patterns, forecasting
|Predictive analytics scheduled training
|===

**Recommended defaults:**

* **Anomaly Detector**: 168h (1 week) - Captures weekly patterns without excessive noise
* **Predictive Analytics**: 720h (30 days) - Captures monthly trends and seasonality

== Data Sources

The platform supports three data source modes for model training:

=== Synthetic Data (`DATA_SOURCE=synthetic`)

**Use case**: Development, testing, CI/CD, when Prometheus is unavailable

* ✅ Fast and reproducible
* ✅ Known anomaly labels for validation
* ✅ No external dependencies
* ⚠️ May not capture real cluster patterns

[source,yaml]
----
params:
  - name: data-source
    value: "synthetic"
----

=== Prometheus Data (`DATA_SOURCE=prometheus`)

**Use case**: Production training with real cluster metrics

* ✅ Real cluster behavior patterns
* ✅ Adapts to actual workload characteristics
* ✅ Improves model accuracy
* ⚠️ Requires Prometheus access
* ⚠️ Real anomalies are rare (<1%)

[source,yaml]
----
params:
  - name: data-source
    value: "prometheus"
----

Training notebooks automatically:

. Fetch real metrics from Prometheus (80% of data)
. Inject synthetic anomalies (20% of data) for balanced training
. Combine datasets for robust model training

=== Hybrid Data (`DATA_SOURCE=hybrid`)

**Use case**: Staging, validation, best of both worlds

* ✅ 50% Prometheus + 50% synthetic
* ✅ Balanced representation
* ✅ Good for validation environments

[source,yaml]
----
params:
  - name: data-source
    value: "hybrid"
----

[TIP]
====
**Recommendation**: Use `prometheus` mode for production scheduled training to ensure models learn real cluster patterns.
====

== Automated Scheduled Training

The platform automatically retrains models weekly via CronJobs:

=== Anomaly Detector (Weekly, Sunday 2 AM UTC)

[source,bash,role=execute]
----
# View CronJob configuration
oc get cronjob weekly-anomaly-detector-training -n {namespace} -o yaml
----

[source,bash,role=execute]
----
# View recent training runs
oc get pipelineruns -n {namespace} -l model-name=anomaly-detector
----

[source,bash,role=execute]
----
# Check latest training job
tkn pipelinerun logs -n {namespace} $(oc get pipelinerun -n {namespace} \
  -l model-name=anomaly-detector --sort-by=.metadata.creationTimestamp -o name | tail -1)
----

=== Predictive Analytics (Weekly, Sunday 3 AM UTC)

[source,bash,role=execute]
----
# View CronJob configuration
oc get cronjob weekly-predictive-analytics-training -n {namespace} -o yaml
----

[source,bash,role=execute]
----
# View recent training runs
oc get pipelineruns -n {namespace} -l model-name=predictive-analytics
----

== Monitoring Training Runs

=== Check Pipeline Status

[source,bash,role=execute]
----
# List all pipeline runs
tkn pipelinerun list -n {namespace}
----

[source,bash,role=execute]
----
# Watch specific run (replace with actual name)
tkn pipelinerun logs train-anomaly-detector-abc123 -f -n {namespace}
----

=== Verify Model Deployment

[source,bash,role=execute]
----
# Check InferenceService status
oc get inferenceservice anomaly-detector -n {namespace}
----

[source,bash,role=execute]
----
# Check predictor pod status
oc get pods -l serving.kserve.io/inferenceservice=anomaly-detector \
  -n {namespace}
----

[source,bash,role=execute]
----
# View model file details
oc exec -n {namespace} deployment/model-troubleshooting-utilities -- \
  ls -lh /mnt/models/anomaly-detector/model.pkl
----

=== Test Model Endpoint

[source,bash,role=execute]
----
# Get predictor pod IP
PREDICTOR_IP=$(oc get pod -n {namespace} \
  -l serving.kserve.io/inferenceservice=anomaly-detector \
  -o jsonpath='{.items[0].status.podIP}')

# Test anomaly detector
curl -X POST http://${PREDICTOR_IP}:8080/v1/models/anomaly-detector:predict \
  -H 'Content-Type: application/json' \
  -d '{"instances": [[0.5, 0.6, 0.4, 0.3, 0.8]]}'
----

== Pre-Training Health Check

Before training, let's check the current state of the ML models. During platform deployment, the NotebookValidationJobs automatically executed the training notebooks and saved model artifacts to the shared storage. However, due to deployment timing, the predictor pods may have started before the models were written -- meaning the models exist on disk but aren't loaded by the serving containers.

=== Step 1: Check Which Models Are Loaded

Query each predictor to see if it has a model registered:

[source,bash,role=execute]
----
# Check anomaly-detector
ANOMALY_IP=$(oc get pod -n {namespace} \
  -l serving.kserve.io/inferenceservice=anomaly-detector \
  -o jsonpath='{.items[0].status.podIP}')

echo "=== Anomaly Detector Models ==="
oc exec -n self-healing-platform deployment/coordination-engine -- \
  curl -s "http://${ANOMALY_IP}:8080/v1/models"
----

[source,bash,role=execute]
----
# Check predictive-analytics
PREDICT_IP=$(oc get pod -n {namespace} \
  -l serving.kserve.io/inferenceservice=predictive-analytics \
  -o jsonpath='{.items[0].status.podIP}')

echo "=== Predictive Analytics Models ==="
oc exec -n self-healing-platform deployment/coordination-engine -- \
  curl -s "http://${PREDICT_IP}:8080/v1/models"
----

If both return `{"models":[]}`, the predictor pods started before the training notebooks finished writing the model files. This is expected on a fresh deployment.

=== Step 2: Verify Model Files Exist on Storage

Even if the predictors haven't loaded the models, the files should exist on the shared PVC:

[source,bash,role=execute]
----
# Check model artifacts on the shared PVC
for model in anomaly-detector predictive-analytics; do
  echo "=== ${model} ==="
  oc exec -n {namespace} \
    $(oc get pod -n {namespace} \
      -l serving.kserve.io/inferenceservice=anomaly-detector \
      -o jsonpath='{.items[0].metadata.name}') -- \
    ls -lh /mnt/models/${model}/ 2>/dev/null || echo "Directory not found"
done
----

You should see `model.pkl` files in each directory. These were trained on synthetic data by the NotebookValidationJobs during deployment.

=== Step 3: Check Predictor Logs

If the models aren't loaded, the predictor logs will show why:

[source,bash,role=execute]
----
oc logs -n {namespace} \
  -l serving.kserve.io/inferenceservice=anomaly-detector \
  --tail=5
----

A common message is `failed to locate model file` -- this means the predictor attempted to load the model at startup before the file was written.

[NOTE]
====
**Why does this happen?** The InferenceService predictor pods (sync-wave 2) start before the NotebookValidationJobs (sync-wave 3+) complete training. The KServe sklearn server loads models only at startup and does not retry. After the training notebooks finish and write the model files, the predictors need to be restarted to pick them up.

In the next section, you'll retrain the models using Tekton pipelines -- which will write fresh model files and restart the predictors automatically.
====

== Hands-On Exercise: Train Your First Model

Now that you've seen the current model state, let's train the anomaly detector with a short training window:

=== Step 1: Start Training

[source,bash,role=execute]
----
oc create -f - <<EOF
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: workshop-train-
  namespace: {namespace}
  labels:
    workshop: self-healing
spec:
  pipelineRef:
    name: model-training-pipeline
  params:
    - name: model-name
      value: "anomaly-detector"
    - name: notebook-path
      value: "notebooks/02-anomaly-detection/01-isolation-forest-implementation.ipynb"
    - name: data-source
      value: "synthetic"
    - name: training-hours
      value: "24"
    - name: inference-service-name
      value: "anomaly-detector"
    - name: git-url
      value: "{platform_repo}.git"
    - name: git-ref
      value: "main"
  timeout: 15m
EOF
----

=== Step 2: Watch the Training

[source,bash,role=execute]
----
# Get the pipeline run name
PIPELINERUN=$(oc get pipelinerun -n {namespace} -l workshop=self-healing \
  --sort-by=.metadata.creationTimestamp -o name | tail -1)

echo "Watching: $PIPELINERUN"
tkn pipelinerun logs $PIPELINERUN -f -n {namespace}
----

=== Step 3: Verify the Model

Once training completes:

[source,bash,role=execute]
----
# Check InferenceService
oc get inferenceservice anomaly-detector -n {namespace}
----

Expected output:
[source]
----
NAME               URL                                              READY
anomaly-detector   http://anomaly-detector-predictor-default.svc    True
----

== Troubleshooting

=== Model Training Fails

**Symptoms**: Pipeline run fails, NotebookValidationJob shows error

**Diagnosis**:
[source,bash,role=execute]
----
# Check pipeline logs
tkn pipelinerun logs <pipelinerun-name> -f -n {namespace}

# Check NotebookValidationJob status
oc get notebookvalidationjobs -n {namespace}
oc describe notebookvalidationjob <job-name> -n {namespace}
----

**Common causes**:

* Insufficient memory (increase `memoryLimit`)
* Prometheus unavailable (check connectivity)
* Git repository inaccessible (verify URL)
* Notebook syntax errors (test locally first)

=== Model Not Loaded (Empty Model Registry)

**Symptoms**: Predictor pod is Running, InferenceService shows Ready, but `GET /v1/models` returns `{"models":[]}` and predictions return `ModelNotFound`

**Diagnosis**:
[source,bash,role=execute]
----
# Check if models are registered
PREDICTOR_IP=$(oc get pod -n {namespace} \
  -l serving.kserve.io/inferenceservice=anomaly-detector \
  -o jsonpath='{.items[0].status.podIP}')
oc exec -n self-healing-platform deployment/coordination-engine -- \
  curl -s "http://${PREDICTOR_IP}:8080/v1/models"

# Check predictor logs for startup errors
oc logs -n {namespace} \
  -l serving.kserve.io/inferenceservice=anomaly-detector \
  --tail=10
----

**Cause**: The predictor pod started before model files were written to the shared PVC. The KServe sklearn server loads models once at startup and does not retry.

**Fix**: Retrain the model (which writes a new model file and triggers a predictor restart), or manually restart the predictor:

[source,bash,role=execute]
----
oc rollout restart deployment/anomaly-detector-predictor -n {namespace}
oc rollout restart deployment/predictive-analytics-predictor -n {namespace}
----

=== Model Won't Load

**Symptoms**: Predictor pod crashes, OOMKilled, CrashLoopBackOff

**Diagnosis**:
[source,bash,role=execute]
----
# Check predictor pod logs
oc logs -n {namespace} \
  -l serving.kserve.io/inferenceservice=anomaly-detector

# Check model file exists
oc exec -n {namespace} deployment/model-troubleshooting-utilities -- \
  ls -lh /mnt/models/anomaly-detector/
----

**Common causes**:

* Model file corrupted (retrain model)
* Model too large (increase predictor memory)
* Incompatible sklearn version (check runtime image)

=== Prometheus Data Issues

**Symptoms**: Training falls back to synthetic data

[IMPORTANT]
====
OpenShift Prometheus requires **bearer token authentication** over HTTPS port 9091.
====

**Diagnosis**:
[source,bash,role=execute]
----
# Check Prometheus accessibility with bearer token
oc exec -n {namespace} deployment/model-troubleshooting-utilities -- sh -c '
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
curl -sk -H "Authorization: Bearer $TOKEN" \
  "https://prometheus-k8s.openshift-monitoring.svc:9091/api/v1/status/config" | head -c 200
'
----

== Summary

In this module, you learned:

* ✅ How to train models manually with PipelineRuns
* ✅ Different training time windows and when to use them
* ✅ Data sources: synthetic, prometheus, hybrid
* ✅ How to monitor training and verify deployments
* ✅ Troubleshooting common training issues

== Next Steps

Now that you have trained models, let's use them!

In **xref:module-02.adoc[Module 2: End-to-End Self-Healing with Lightspeed]**, you'll:

* Chat with your cluster using natural language
* Deploy sample applications
* Predict future resource usage
* Break things on purpose and watch AI fix them!
